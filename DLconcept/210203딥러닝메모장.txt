210203 모두를 위한 딥러닝 강좌 시즌

#### Lec 04 - multi-variable Linear Regression

* 지난 수업 복습 (one - variable)
: Hypothesis, Cost function, Gradient descent algorithm

* multi-variable 
ex. 시험 n 개의 점수 

## Hypothesis
H(x1, x2, x3, ...) = w1x1 + w2x2 + w3x3 + ... + b

## Cost function
cost(w,b) = (H(x1, x2, x3) - y)^2의 평균

-> n개면 수식이 엄청 길어짐
by Matrix (행렬) - Dot Product (내적)

# Hypothesis using Matrix
-> H(X) = XW : 행렬의 곱셈으로 간단하게 표현 가능

* H(x1,x2,x3) = x1w1 + x2w2 + x3w3 라고 하자 (b=0)

* Many x(data 갯수) instances (row 행 갯수)
-> Matrix를 쓰면 각각 계산이 아니라 한번에 input 넣고 연산 가능
ex. XW = H(X)
    [5,3] [3,1] [3,1] -> X의 instance는 5개, Weight 크기는 [3,1]
-> instance 많으면 n (None)으로 설정


#### Lab 04-1 - multi-variable Linear Regression을 TensorFlow에서 구현하기

* H(x1,x2,x3) = x1w1 + x2w2 + x3w3 -> Y를 예측해보자

-> training 결과 : Cost가 감소하며 Prediction 값이 Y에 수렴한다
-> instance 많으면 복잡 -> Matrix로 구현하자


#### Lab 04-2 - TensorFlow로 파일에서 Data 읽어오기















