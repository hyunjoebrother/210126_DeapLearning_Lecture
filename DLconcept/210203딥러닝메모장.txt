210203 모두를 위한 딥러닝 강좌 시즌

#### Lec 04 - multi-variable Linear Regression

* 지난 수업 복습 (one - variable)
: Hypothesis, Cost function, Gradient descent algorithm

* multi-variable 
ex. 시험 n 개의 점수 

## Hypothesis
H(x1, x2, x3, ...) = w1x1 + w2x2 + w3x3 + ... + b

## Cost function
cost(w,b) = (H(x1, x2, x3) - y)^2의 평균

-> n개면 수식이 엄청 길어짐
by Matrix (행렬) - Dot Product (내적)

# Hypothesis using Matrix
-> H(X) = XW : 행렬의 곱셈으로 간단하게 표현 가능

* H(x1,x2,x3) = x1w1 + x2w2 + x3w3 라고 하자 (b=0)

* Many x(data 갯수) instances (row 행 갯수)
-> Matrix를 쓰면 각각 계산이 아니라 한번에 input 넣고 연산 가능
ex. XW = H(X)
    [5,3] [3,1] [3,1] -> X의 instance는 5개, Weight 크기는 [3,1]
-> instance 많으면 n (None)으로 설정


#### Lab 04-1 - multi-variable Linear Regression을 TensorFlow에서 구현하기

* H(x1,x2,x3) = x1w1 + x2w2 + x3w3 -> Y를 예측해보자

-> training 결과 : Cost가 감소하며 Prediction 값이 Y에 수렴한다
-> instance 많으면 복잡 -> Matrix로 구현하자


210204 모두를 위한 딥러닝 강좌 시즌

#### Lab 04-2 - TensorFlow로 파일에서 Data 읽어오기
: data가 많아지니까 코드에 직접 쓰기 힘들다 
-> numpy에서 Slicing 이용

## Queue Runners 
: numpy로도 힘든 data -> Queue에 필요한만큼 쌓고 꺼내서 쓴다

step1 : file들을 리스트로 만든다
step2 : file을 읽어오는 reader 
step3 : value값을 이해(파싱)하기 위한 decode_csv 가져오고 data type 지정

tf.train.batch : x_data와 y_data 가져오고 size 지정
batch는 tensor므로 sess.run으로 실행시키고, feed-dict로 값을 넘겨준다


210205 정통 21학번 신입생 정모로 pass

210206 모두를 위한 딥러닝 강좌 시즌


210207 모두를 위한 딥러닝 강좌 시즌
#### Lec 05-1 - Logistic Classification의 가설 함수 정의

* 복습 - Regression (결과값 y 예측하기)
Hypothesis -> Cost -> Gradient descent


## (Binary) Classification
: 단순 숫자 예측이 아닌 2개 중 하나를 정해줌
ex. Spam Email Detection spam/ham, Facebook Feed show/hide
-> 0/1 encoding
ex. spam(1) or ham(0), show(1) or hide(0)


## Pass(1)/Fail(0) based on study hours
-> How to apply Classification Algorithm?

* Linear Regression?
: 0.5 기준으로 가정 check 가능?
-> 하지만 예외가 발생할 수 있다 (많이 공부했는데 fail)
-> 합격/불합격 checking 기준이 변하고 y값이 0/1 안 나올 수도 있다.
-> modeling은 간단하지만 쉽지 않다...

-> y값을 0~1 사이로 만들어주는 함수 g(z) : sigmoid function
: z=WX라 하면, 새로운 H(x) = g(z) 라고 가정하자

# Logistic Hypothesis
: Classification의 가설 함수가 된다


210208 모두를 위한 딥러닝 강좌 시즌
#### Lec 05-2 - Logistic Regression의 cost 함수 정의

## Cost Function
: 예측값이 비슷할수록 cost 값이 작아진다
-> 지난 단원에서 H(x) 값이 0~1이도록 만들었음
-> 그래프가 매끄럽지 않아서 minimum 값을 찾기 힘들다
-> 범위마다 달라서 총 최솟값(global min)이 아니라 구간별(local)마다 달라진다

## New Cost function for Logistic
: cost(W) = C(H(x), y) 값의 평균으로 하자
-> 형태가 log함수
if) y=1 -> H(x) = 1이면 cost = 0, H(x) = 0이면 cost 값 무한에 가까워짐 (잘못 예측)
if) y=0 -> H(x) = 0이면 cost = 0, H(x) = 1이면 cost 값 무한에 가까워짐 (잘못 예측)

## Minimize Cost
-> By Gradient descent algorithm
-> 똑같이 미분하고 alpha값으로 learning rate 후 값을 update
* tf.train.GradientDescentOptimizer(a)라는 라이브러리 사용한다


#### Lab 05 - TensorFlow로 Logistic Classification의 구현하기

## Training 결과
- Cost 함수가 점점 0에 가까워짐
- Hypothesis (y값)은 점점 1에 가까워짐
- Correct(Y) -> Hypothesis가 0.5보다 크면 print 1
- Accuracy : 1에 가까울수록 정확

## Classifying diabetes 당뇨병
: csv 파일을 불러오고 학습 후에 예측해보자









