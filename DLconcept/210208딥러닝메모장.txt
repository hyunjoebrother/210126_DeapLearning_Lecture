####210208 모두를 위한 딥러닝 강좌 시즌
## Lec 06-1 - Softmax Regression 기본 개념 소개

# Logistic Regression & Classification
: Linear한 Hypothesis로 check -> Sigmoid(Logistic) function
-> Prediction (Y hat) & Accuracy check

# Multinomial Classification
: 여러 개의 class가 있을 때 예측
ex. 공부 시간 hours, 출석 attendence  -> 학점 grade (A, B, C)
-> class : 학점 A/B/C or NOT
-> 독립된 Matrix 서로 곱해서 한번에 Hypothesis 계산 가능
-> 각각 sigmoid 적용하여 Hypothesis 계산하는데 귀찮다? -> 다음 단원


## Lec 06-2 - Softmax classifier의 cost 함수

# Sigmoid : 0~1 사이 값으로 줄어준다
# Softmax : 나오는 모든 probabilities 합이 1이 나오도록 계산
-> "ONE-HOT Encoding" 기법으로 확률 가장 큰 값 -> 하나만 출력

# Cross Entropy Cost Function
: 예측이 틀리면 cost가 큰 값
D(S, L) = - (log함수 합) -> 실질적으로 logistic cost와 같은 꼴
-> 전체 거리 distance 계산 후 평균을 낸다

# Gradient descent
: In same way... 미분하고 learning rate 계산



## Lab 06-1 - Tensorflow로 Softmax Classificaiton의 구현하기

-> softmax 함수가 있음
-> 어떤 label이 나올지 확률을 구해서 예측하자

# ONE-HOT Encoding
: 여러 label 중에서 하나만 check (HOT) -> 1로 만든다
-> y_data -> 2/1/0 이런 식으로 label check

# arg_max 
: 확률 가장 큰 값을 check -> 몇 번째 argument가 가장 max인지


####210211 모두를 위한 딥러닝 강좌 시즌
## Lab 06-2 - Tensorflow로 Fancy Softmax Classificaiton의 구현하기

# softmax_cross_entropy_with_logits 함수
* logits (=score) -> softmax 통과시키면 최종 확률 hypothesis 계산
-> ONE-HOT인 Y값 label과 logits 값을 함수에 넣어주고 평균을 구한다


# Animal Classification -> 어떤 특징을 통해서 종을 분류

# tf.one_hot & tf.reshape 함수
Y를 one_hot으로 바꿔주자 - 0~6, class 7개 중에서 check
-> reshape로 rank하나 줄여서 우리가 원하는 값으로 바꿔준다

# Train & Predict 결과
: step 갈수록 Loss가 감소, Acc가 높아진다
예측이 맞으면 True 출력, 예측값과 결과값 출력된다 





