####210208 모두를 위한 딥러닝 강좌 시즌
## Lec 06-1 - Softmax Regression 기본 개념 소개

# Logistic Regression & Classification
: Linear한 Hypothesis로 check -> Sigmoid(Logistic) function
-> Prediction (Y hat) & Accuracy check

# Multinomial Classification
: 여러 개의 class가 있을 때 예측
ex. 공부 시간 hours, 출석 attendence  -> 학점 grade (A, B, C)
-> class : 학점 A/B/C or NOT
-> 독립된 Matrix 서로 곱해서 한번에 Hypothesis 계산 가능
-> 각각 sigmoid 적용하여 Hypothesis 계산하는데 귀찮다? -> 다음 단원


## Lec 06-2 - Softmax classifier의 cost 함수

# Sigmoid : 0~1 사이 값으로 줄어준다
# Softmax : 나오는 모든 probabilities 합이 1이 나오도록 계산
-> "ONE-HOT Encoding" 기법으로 확률 가장 큰 값 -> 하나만 출력

# Cross Entropy Cost Function
: 예측이 틀리면 cost가 큰 값
D(S, L) = - (log함수 합) -> 실질적으로 logistic cost와 같은 꼴
-> 전체 거리 distance 계산 후 평균을 낸다

# Gradient descent
: In same way... 미분하고 learning rate 계산



## Lab 06-1 - Tensorflow로 Softmax Classificaiton의 구현하기

-> softmax 함수가 있음
-> 어떤 label이 나올지 확률을 구해서 예측하자

# ONE-HOT Encoding
: 여러 label 중에서 하나만 check (HOT) -> 1로 만든다
-> y_data -> 2/1/0 이런 식으로 label check

# arg_max 
: 확률 가장 큰 값을 check -> 몇 번째 argument가 가장 max인지


## Lab 06-2 - Tensorflow로 Fancy Softmax Classificaiton의 구현하기

# softmax_cross_entropy_with_logits 함수
* logits (=score) -> softmax 통과시키면 최종 확률 hypothesis 계산

 


